{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T20:02:55.989107Z","iopub.status.busy":"2024-04-20T20:02:55.988743Z","iopub.status.idle":"2024-04-20T20:02:56.357249Z","shell.execute_reply":"2024-04-20T20:02:56.356275Z","shell.execute_reply.started":"2024-04-20T20:02:55.989080Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T20:03:07.596802Z","iopub.status.busy":"2024-04-20T20:03:07.595859Z","iopub.status.idle":"2024-04-20T20:03:07.870034Z","shell.execute_reply":"2024-04-20T20:03:07.868902Z","shell.execute_reply.started":"2024-04-20T20:03:07.596744Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 23406 entries, 0 to 23405\n","Data columns (total 2 columns):\n"," #   Column   Non-Null Count  Dtype \n","---  ------   --------------  ----- \n"," 0   English  23406 non-null  object\n"," 1   Arabic   23406 non-null  object\n","dtypes: object(2)\n","memory usage: 365.8+ KB\n"]}],"source":["df = pd.read_csv('/kaggle/input/translation/translation_train.csv') \n","df.head(10)\n","df.info()\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T20:03:09.366380Z","iopub.status.busy":"2024-04-20T20:03:09.365936Z","iopub.status.idle":"2024-04-20T20:03:13.186623Z","shell.execute_reply":"2024-04-20T20:03:13.185569Z","shell.execute_reply.started":"2024-04-20T20:03:09.366348Z"},"trusted":true},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["from io import open\n","import unicodedata\n","import re\n","import random\n","\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","\n","import numpy as np\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T20:03:13.188840Z","iopub.status.busy":"2024-04-20T20:03:13.188379Z","iopub.status.idle":"2024-04-20T20:03:13.196748Z","shell.execute_reply":"2024-04-20T20:03:13.195751Z","shell.execute_reply.started":"2024-04-20T20:03:13.188806Z"},"trusted":true},"outputs":[],"source":["SOS_token = 0\n","EOS_token = 1\n","\n","class Lang:\n","    def __init__(self, name):\n","        self.name = name\n","        self.word2index = {}\n","        self.word2count = {}\n","        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n","        self.n_words = 2  # Count SOS and EOS\n","\n","    def addSentence(self, sentence):\n","        for word in sentence.split(' '):\n","            self.addWord(word)\n","\n","    def addWord(self, word):\n","        if word not in self.word2index:\n","            self.word2index[word] = self.n_words\n","            self.word2count[word] = 1\n","            self.index2word[self.n_words] = word\n","            self.n_words += 1\n","        else:\n","            self.word2count[word] += 1"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T20:03:13.198182Z","iopub.status.busy":"2024-04-20T20:03:13.197901Z","iopub.status.idle":"2024-04-20T20:03:13.209185Z","shell.execute_reply":"2024-04-20T20:03:13.208240Z","shell.execute_reply.started":"2024-04-20T20:03:13.198158Z"},"trusted":true},"outputs":[],"source":["# Turn a Unicode string to plain ASCII, thanks to\n","# https://stackoverflow.com/a/518232/2809427\n","def unicodeToAscii(s):\n","    return ''.join(\n","        c for c in unicodedata.normalize('NFD', s)\n","        if unicodedata.category(c) != 'Mn'\n","    )"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T20:03:13.212378Z","iopub.status.busy":"2024-04-20T20:03:13.211699Z","iopub.status.idle":"2024-04-20T20:03:13.245914Z","shell.execute_reply":"2024-04-20T20:03:13.244937Z","shell.execute_reply.started":"2024-04-20T20:03:13.212337Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["مرحبا بكم في العالم العربي\n"]}],"source":["import unicodedata\n","import regex  # Better support for Unicode regular expressions\n","\n","def normalizeArabic(text):\n","\n","    text = unicodeToAscii(text.lower().strip())\n","    # Normalize Arabic characters\n","    text = regex.sub(r'[\\p{Mn}\\p{Sk}]+', '', unicodedata.normalize('NFKD', text))\n","\n","    # Remove non-letter, non-space characters\n","    text = regex.sub(r'[^\\p{L}\\s]', '', text)\n","\n","    # Normalize whitespace\n","    text = regex.sub(r'\\s+', ' ', text)\n","\n","    return text.strip()\n","\n","# Example usage\n","arabic_text = \"مرحباً بكم في العالم العربي! %&&\"\n","normalized_text = normalizeArabic(arabic_text)\n","print(normalized_text)  # Output: \"مرحبا بكم في العالم العربي\""]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T20:03:13.247256Z","iopub.status.busy":"2024-04-20T20:03:13.246970Z","iopub.status.idle":"2024-04-20T20:03:13.255257Z","shell.execute_reply":"2024-04-20T20:03:13.254352Z","shell.execute_reply.started":"2024-04-20T20:03:13.247233Z"},"trusted":true},"outputs":[],"source":["import csv\n","\n","def readLangs(lang1, lang2, reverse=False):\n","    print(\"Reading lines...\")\n","\n","    # Open the CSV file\n","    with open('/kaggle/input/translation/translation_train.csv', newline='', encoding='utf-8') as csvfile:\n","        # Create a CSV reader object\n","        reader = csv.reader(csvfile)\n","        \n","        # Read the rows of the CSV file\n","        lines = [row for row in reader]\n","\n","    # Split every line into pairs and normalize\n","    pairs = [[normalizeArabic(s) for s in l] for l in lines]\n","\n","    # Reverse pairs, make Lang instances\n","    if reverse:\n","        pairs = [list(reversed(p)) for p in pairs]\n","        input_lang = Lang(lang2)\n","        print(input_lang)\n","        output_lang = Lang(lang1)\n","        print(output_lang)\n","    else:\n","        input_lang = Lang(lang1)\n","        output_lang = Lang(lang2)\n","\n","    return input_lang, output_lang, pairs\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T20:03:13.257285Z","iopub.status.busy":"2024-04-20T20:03:13.256590Z","iopub.status.idle":"2024-04-20T20:03:13.265652Z","shell.execute_reply":"2024-04-20T20:03:13.264769Z","shell.execute_reply.started":"2024-04-20T20:03:13.257249Z"},"trusted":true},"outputs":[],"source":["# some filtering based on sentence length and sentence start prefix\n","MAX_LENGTH = 10\n","\n","eng_prefixes = (\n","    \"i am \", \"i m \",\n","    \"he is\", \"he s \",\n","    \"she is\", \"she s \",\n","    \"you are\", \"you re \",\n","    \"we are\", \"we re \",\n","    \"they are\", \"they re \",\"This\",\"That\"\n",")\n","\n","ara_prefixes = ( 'هذه','انا', 'لا')\n","\n","def filterPair(p):\n","    return len(p[0].split(' ')) < MAX_LENGTH and \\\n","        len(p[1].split(' ')) < MAX_LENGTH and \\\n","        p[1].startswith(ara_prefixes) # turning off filtering \n","\n","\n","def filterPairs(pairs):\n","    return [pair for pair in pairs if filterPair(pair)]"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T20:03:13.267748Z","iopub.status.busy":"2024-04-20T20:03:13.267305Z","iopub.status.idle":"2024-04-20T20:03:17.650838Z","shell.execute_reply":"2024-04-20T20:03:17.648766Z","shell.execute_reply.started":"2024-04-20T20:03:13.267701Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading lines...\n","Read 23407 sentence pairs\n","Trimmed to 1058 sentence pairs\n","Counting words...\n","Counted words:\n","input lang is :english\n","english 980\n","output lang is :arabic\n","arabic 1485\n","['i didnt see the need for it', 'لا ارى لذلك حاجة']\n"]}],"source":["def prepareData(lang1, lang2, reverse=False):\n","    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n","    print(\"Read %s sentence pairs\" % len(pairs))\n","    pairs = filterPairs(pairs)\n","    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n","    print(\"Counting words...\")\n","    for pair in pairs:\n","        input_lang.addSentence(pair[0])\n","        output_lang.addSentence(pair[1])\n","    print(\"Counted words:\")\n","    print(f'input lang is :{input_lang.name}')\n","    print(input_lang.name, input_lang.n_words)\n","    print(f'output lang is :{output_lang.name}')\n","    print(output_lang.name, output_lang.n_words)\n","    return input_lang, output_lang, pairs\n","\n","input_lang, output_lang, pairs = prepareData('english', 'arabic', False)\n","print(random.choice(pairs))"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T20:03:17.652431Z","iopub.status.busy":"2024-04-20T20:03:17.652132Z","iopub.status.idle":"2024-04-20T20:03:17.657847Z","shell.execute_reply":"2024-04-20T20:03:17.656683Z","shell.execute_reply.started":"2024-04-20T20:03:17.652406Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['im one of the good guys', 'انا احد الرجال الصالحين']\n"]}],"source":["# lets take an example\n","print(random.choice(pairs))"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T20:03:17.659621Z","iopub.status.busy":"2024-04-20T20:03:17.659274Z","iopub.status.idle":"2024-04-20T20:03:17.668018Z","shell.execute_reply":"2024-04-20T20:03:17.666874Z","shell.execute_reply.started":"2024-04-20T20:03:17.659596Z"},"trusted":true},"outputs":[],"source":["class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n","        super(EncoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(input_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n","        self.dropout = nn.Dropout(dropout_p)\n","\n","    def forward(self, input):\n","        embedded = self.dropout(self.embedding(input))\n","        output, hidden = self.gru(embedded)\n","        return output, hidden"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T20:03:17.671964Z","iopub.status.busy":"2024-04-20T20:03:17.671626Z","iopub.status.idle":"2024-04-20T20:03:17.685408Z","shell.execute_reply":"2024-04-20T20:03:17.684451Z","shell.execute_reply.started":"2024-04-20T20:03:17.671939Z"},"trusted":true},"outputs":[],"source":["class DecoderRNN(nn.Module):\n","    def __init__(self, hidden_size, output_size):\n","        super(DecoderRNN, self).__init__()\n","        self.embedding = nn.Embedding(output_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n","        self.out = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n","        batch_size = encoder_outputs.size(0)\n","        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n","        decoder_hidden = encoder_hidden\n","        decoder_outputs = []\n","\n","        for i in range(MAX_LENGTH):\n","            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n","            decoder_outputs.append(decoder_output)\n","\n","            if target_tensor is not None:\n","                # Teacher forcing: Feed the target as the next input\n","                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n","            else:\n","                # Without teacher forcing: use its own predictions as the next input\n","                _, topi = decoder_output.topk(1)\n","                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n","\n","        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n","        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n","        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n","\n","    def forward_step(self, input, hidden):\n","        output = self.embedding(input)\n","        output = F.relu(output)\n","        output, hidden = self.gru(output, hidden)\n","        output = self.out(output)\n","        return output, hidden"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T20:03:17.687041Z","iopub.status.busy":"2024-04-20T20:03:17.686696Z","iopub.status.idle":"2024-04-20T20:03:17.698838Z","shell.execute_reply":"2024-04-20T20:03:17.697597Z","shell.execute_reply.started":"2024-04-20T20:03:17.687011Z"},"trusted":true},"outputs":[],"source":["class BahdanauAttention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(BahdanauAttention, self).__init__()\n","        self.Wa = nn.Linear(hidden_size, hidden_size)\n","        self.Ua = nn.Linear(hidden_size, hidden_size)\n","        self.Va = nn.Linear(hidden_size, 1)\n","\n","    def forward(self, query, keys):\n","        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n","        scores = scores.squeeze(2).unsqueeze(1)\n","\n","        weights = F.softmax(scores, dim=-1)\n","        context = torch.bmm(weights, keys)\n","\n","        return context, weights\n"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T20:03:17.700812Z","iopub.status.busy":"2024-04-20T20:03:17.700456Z","iopub.status.idle":"2024-04-20T20:03:17.722790Z","shell.execute_reply":"2024-04-20T20:03:17.721585Z","shell.execute_reply.started":"2024-04-20T20:03:17.700780Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class AttnDecoderRNN(nn.Module):\n","    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n","        super(AttnDecoderRNN, self).__init__()\n","        self.embedding = nn.Embedding(output_size, hidden_size)\n","        self.attention = BahdanauAttention(hidden_size)\n","        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n","        self.out = nn.Linear(hidden_size, output_size)\n","        self.dropout = nn.Dropout(dropout_p)\n","\n","    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None, teacher_forcing_ratio=0.5):\n","        batch_size = encoder_outputs.size(0)\n","        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n","        decoder_hidden = encoder_hidden\n","        decoder_outputs = []\n","        attentions = []\n","\n","        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n","\n","        for i in range(MAX_LENGTH):\n","            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n","                decoder_input, decoder_hidden, encoder_outputs\n","            )\n","            decoder_outputs.append(decoder_output)\n","            attentions.append(attn_weights)\n","\n","            if use_teacher_forcing and target_tensor is not None:\n","                # Teacher forcing: Feed the target as the next input\n","                decoder_input = target_tensor[:, i].unsqueeze(1)\n","            else:\n","                # Without teacher forcing: use its own predictions as the next input\n","                _, topi = decoder_output.topk(1)\n","                decoder_input = topi.squeeze(-1).detach() # detach from history as input\n","\n","        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n","        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n","        attentions = torch.cat(attentions, dim=1)\n","\n","        return decoder_outputs, decoder_hidden, attentions\n","\n","    def forward_step(self, input, hidden, encoder_outputs):\n","        embedded = self.dropout(self.embedding(input))\n","\n","        query = hidden.permute(1, 0, 2)\n","        context, attn_weights = self.attention(query, encoder_outputs)\n","        input_gru = torch.cat((embedded, context), dim=2)\n","\n","        output, hidden = self.gru(input_gru, hidden)\n","        output = self.out(output)\n","\n","        return output, hidden, attn_weights\n"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T20:03:17.725910Z","iopub.status.busy":"2024-04-20T20:03:17.724155Z","iopub.status.idle":"2024-04-20T20:03:17.738060Z","shell.execute_reply":"2024-04-20T20:03:17.737013Z","shell.execute_reply.started":"2024-04-20T20:03:17.725884Z"},"trusted":true},"outputs":[],"source":["def indexesFromSentence(lang, sentence):\n","    return [lang.word2index[word] for word in sentence.split(' ')]\n","\n","def tensorFromSentence(lang, sentence):\n","    indexes = indexesFromSentence(lang, sentence)\n","    indexes.append(EOS_token)\n","    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n","\n","def tensorsFromPair(pair):\n","    input_tensor = tensorFromSentence(input_lang, pair[0])\n","    target_tensor = tensorFromSentence(output_lang, pair[1])\n","    return (input_tensor, target_tensor)\n","\n","def get_dataloader(batch_size):\n","    input_lang, output_lang, pairs = prepareData('eng', 'ara', False)\n","\n","    n = len(pairs)\n","    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n","    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n","\n","    for idx, (inp, tgt) in enumerate(pairs):\n","        inp_ids = indexesFromSentence(input_lang, inp)\n","        tgt_ids = indexesFromSentence(output_lang, tgt)\n","        inp_ids.append(EOS_token)\n","        tgt_ids.append(EOS_token)\n","        input_ids[idx, :len(inp_ids)] = inp_ids\n","        target_ids[idx, :len(tgt_ids)] = tgt_ids\n","\n","    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n","                               torch.LongTensor(target_ids).to(device))\n","\n","    train_sampler = RandomSampler(train_data)\n","    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","    return input_lang, output_lang, train_dataloader"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T20:03:17.739417Z","iopub.status.busy":"2024-04-20T20:03:17.739093Z","iopub.status.idle":"2024-04-20T20:03:17.750398Z","shell.execute_reply":"2024-04-20T20:03:17.749520Z","shell.execute_reply.started":"2024-04-20T20:03:17.739388Z"},"trusted":true},"outputs":[],"source":["def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n","          decoder_optimizer, criterion):\n","\n","    total_loss = 0\n","    for data in dataloader:\n","        input_tensor, target_tensor = data\n","\n","        encoder_optimizer.zero_grad()\n","        decoder_optimizer.zero_grad()\n","\n","        encoder_outputs, encoder_hidden = encoder(input_tensor)\n","        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n","\n","        loss = criterion(\n","            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n","            target_tensor.view(-1)\n","        )\n","        loss.backward()\n","\n","        encoder_optimizer.step()\n","        decoder_optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    return total_loss / len(dataloader)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T20:03:17.752031Z","iopub.status.busy":"2024-04-20T20:03:17.751673Z","iopub.status.idle":"2024-04-20T20:03:17.763329Z","shell.execute_reply":"2024-04-20T20:03:17.762333Z","shell.execute_reply.started":"2024-04-20T20:03:17.752003Z"},"trusted":true},"outputs":[],"source":["import time\n","import math\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T20:03:17.765288Z","iopub.status.busy":"2024-04-20T20:03:17.764618Z","iopub.status.idle":"2024-04-20T20:03:17.774965Z","shell.execute_reply":"2024-04-20T20:03:17.774081Z","shell.execute_reply.started":"2024-04-20T20:03:17.765255Z"},"trusted":true},"outputs":[],"source":["def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n","               print_every=100, plot_every=100):\n","    start = time.time()\n","    plot_losses = []\n","    print_loss_total = 0  # Reset every print_every\n","    plot_loss_total = 0  # Reset every plot_every\n","\n","    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n","    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n","    criterion = nn.NLLLoss()\n","\n","    for epoch in range(1, n_epochs + 1):\n","        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n","        print_loss_total += loss\n","        plot_loss_total += loss\n","\n","        if epoch % print_every == 0:\n","            print_loss_avg = print_loss_total / print_every\n","            print_loss_total = 0\n","            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n","                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n","\n","        if epoch % plot_every == 0:\n","            plot_loss_avg = plot_loss_total / plot_every\n","            plot_losses.append(plot_loss_avg)\n","            plot_loss_total = 0\n","\n","    showPlot(plot_losses)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T20:03:17.776692Z","iopub.status.busy":"2024-04-20T20:03:17.776339Z","iopub.status.idle":"2024-04-20T20:03:17.788883Z","shell.execute_reply":"2024-04-20T20:03:17.787760Z","shell.execute_reply.started":"2024-04-20T20:03:17.776662Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","plt.switch_backend('agg')\n","import matplotlib.ticker as ticker\n","import numpy as np\n","\n","def showPlot(points):\n","    plt.figure()\n","    fig, ax = plt.subplots()\n","    # this locator puts ticks at regular intervals\n","    loc = ticker.MultipleLocator(base=0.2)\n","    ax.yaxis.set_major_locator(loc)\n","    plt.plot(points)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T20:03:17.790390Z","iopub.status.busy":"2024-04-20T20:03:17.790057Z","iopub.status.idle":"2024-04-20T20:03:17.799745Z","shell.execute_reply":"2024-04-20T20:03:17.798762Z","shell.execute_reply.started":"2024-04-20T20:03:17.790360Z"},"trusted":true},"outputs":[],"source":["def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n","    with torch.no_grad():\n","        input_tensor = tensorFromSentence(input_lang, sentence)\n","\n","        encoder_outputs, encoder_hidden = encoder(input_tensor)\n","        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n","\n","        _, topi = decoder_outputs.topk(1)\n","        decoded_ids = topi.squeeze()\n","\n","        decoded_words = []\n","        for idx in decoded_ids:\n","            if idx.item() == EOS_token:\n","                decoded_words.append('<EOS>')\n","                break\n","            decoded_words.append(output_lang.index2word[idx.item()])\n","    return decoded_words, decoder_attn"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T20:03:17.801440Z","iopub.status.busy":"2024-04-20T20:03:17.800958Z","iopub.status.idle":"2024-04-20T20:03:17.815511Z","shell.execute_reply":"2024-04-20T20:03:17.814549Z","shell.execute_reply.started":"2024-04-20T20:03:17.801409Z"},"trusted":true},"outputs":[],"source":["def evaluateRandomly(encoder, decoder, n=10):\n","    for i in range(n):\n","        pair = random.choice(pairs)\n","        print('>', pair[0])\n","        print('=', pair[1])\n","        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n","        output_sentence = ' '.join(output_words)\n","        print('<', output_sentence)\n","        print('')"]},{"cell_type":"code","execution_count":23,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-04-20T20:03:17.817380Z","iopub.status.busy":"2024-04-20T20:03:17.817048Z","iopub.status.idle":"2024-04-20T20:26:52.699134Z","shell.execute_reply":"2024-04-20T20:26:52.698025Z","shell.execute_reply.started":"2024-04-20T20:03:17.817350Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading lines...\n","Read 23407 sentence pairs\n","Trimmed to 1058 sentence pairs\n","Counting words...\n","Counted words:\n","input lang is :eng\n","eng 980\n","output lang is :ara\n","ara 1485\n","0m 10s (- 35m 24s) (5 0%) 2.4150\n","0m 17s (- 29m 9s) (10 1%) 1.4797\n","0m 24s (- 27m 3s) (15 1%) 0.8097\n","0m 31s (- 26m 1s) (20 2%) 0.3367\n","0m 38s (- 25m 16s) (25 2%) 0.1622\n","0m 45s (- 24m 46s) (30 3%) 0.1071\n","0m 53s (- 24m 24s) (35 3%) 0.0911\n","1m 0s (- 24m 1s) (40 4%) 0.0809\n","1m 7s (- 23m 43s) (45 4%) 0.0742\n","1m 14s (- 23m 28s) (50 5%) 0.0675\n","1m 21s (- 23m 15s) (55 5%) 0.0651\n","1m 28s (- 23m 2s) (60 6%) 0.0674\n","1m 35s (- 22m 52s) (65 6%) 0.0704\n","1m 42s (- 22m 41s) (70 7%) 0.0661\n","1m 49s (- 22m 30s) (75 7%) 0.0701\n","1m 56s (- 22m 18s) (80 8%) 0.0714\n","2m 3s (- 22m 10s) (85 8%) 0.0646\n","2m 10s (- 22m 1s) (90 9%) 0.0595\n","2m 17s (- 21m 52s) (95 9%) 0.0623\n","2m 24s (- 21m 42s) (100 10%) 0.0594\n","2m 31s (- 21m 34s) (105 10%) 0.0745\n","2m 38s (- 21m 26s) (110 11%) 0.0626\n","2m 46s (- 21m 18s) (115 11%) 0.0621\n","2m 53s (- 21m 9s) (120 12%) 0.0594\n","3m 0s (- 21m 0s) (125 12%) 0.0618\n","3m 7s (- 20m 53s) (130 13%) 0.0579\n","3m 14s (- 20m 45s) (135 13%) 0.0580\n","3m 21s (- 20m 36s) (140 14%) 0.0678\n","3m 28s (- 20m 27s) (145 14%) 0.0576\n","3m 35s (- 20m 19s) (150 15%) 0.0591\n","3m 42s (- 20m 11s) (155 15%) 0.0605\n","3m 49s (- 20m 2s) (160 16%) 0.0569\n","3m 55s (- 19m 54s) (165 16%) 0.0562\n","4m 2s (- 19m 46s) (170 17%) 0.0580\n","4m 9s (- 19m 38s) (175 17%) 0.0571\n","4m 17s (- 19m 30s) (180 18%) 0.0555\n","4m 23s (- 19m 22s) (185 18%) 0.0799\n","4m 30s (- 19m 15s) (190 19%) 0.0670\n","4m 37s (- 19m 7s) (195 19%) 0.0605\n","4m 45s (- 19m 0s) (200 20%) 0.0552\n","4m 52s (- 18m 53s) (205 20%) 0.0559\n","4m 59s (- 18m 46s) (210 21%) 0.0579\n","5m 6s (- 18m 38s) (215 21%) 0.0563\n","5m 13s (- 18m 31s) (220 22%) 0.0581\n","5m 20s (- 18m 24s) (225 22%) 0.0576\n","5m 27s (- 18m 16s) (230 23%) 0.0541\n","5m 34s (- 18m 9s) (235 23%) 0.0605\n","5m 41s (- 18m 2s) (240 24%) 0.0566\n","5m 48s (- 17m 54s) (245 24%) 0.0565\n","5m 55s (- 17m 46s) (250 25%) 0.0561\n","6m 2s (- 17m 39s) (255 25%) 0.0565\n","6m 9s (- 17m 31s) (260 26%) 0.0654\n","6m 16s (- 17m 24s) (265 26%) 0.0584\n","6m 23s (- 17m 16s) (270 27%) 0.0553\n","6m 30s (- 17m 9s) (275 27%) 0.0546\n","6m 37s (- 17m 1s) (280 28%) 0.0551\n","6m 44s (- 16m 54s) (285 28%) 0.0563\n","6m 51s (- 16m 46s) (290 28%) 0.0602\n","6m 58s (- 16m 39s) (295 29%) 0.0557\n","7m 5s (- 16m 32s) (300 30%) 0.0585\n","7m 12s (- 16m 24s) (305 30%) 0.0571\n","7m 19s (- 16m 17s) (310 31%) 0.0567\n","7m 26s (- 16m 9s) (315 31%) 0.0564\n","7m 32s (- 16m 2s) (320 32%) 0.0558\n","7m 39s (- 15m 54s) (325 32%) 0.0606\n","7m 46s (- 15m 47s) (330 33%) 0.0602\n","7m 53s (- 15m 40s) (335 33%) 0.0540\n","8m 0s (- 15m 32s) (340 34%) 0.0532\n","8m 7s (- 15m 25s) (345 34%) 0.0564\n","8m 14s (- 15m 18s) (350 35%) 0.0598\n","8m 21s (- 15m 11s) (355 35%) 0.0554\n","8m 28s (- 15m 4s) (360 36%) 0.0563\n","8m 35s (- 14m 57s) (365 36%) 0.0574\n","8m 42s (- 14m 50s) (370 37%) 0.0551\n","8m 49s (- 14m 43s) (375 37%) 0.0660\n","8m 56s (- 14m 36s) (380 38%) 0.0617\n","9m 4s (- 14m 29s) (385 38%) 0.0599\n","9m 11s (- 14m 22s) (390 39%) 0.0652\n","9m 18s (- 14m 15s) (395 39%) 0.0582\n","9m 25s (- 14m 8s) (400 40%) 0.0538\n","9m 32s (- 14m 1s) (405 40%) 0.0519\n","9m 39s (- 13m 53s) (410 41%) 0.0581\n","9m 46s (- 13m 46s) (415 41%) 0.0541\n","9m 53s (- 13m 39s) (420 42%) 0.0561\n","10m 0s (- 13m 32s) (425 42%) 0.0529\n","10m 7s (- 13m 24s) (430 43%) 0.0541\n","10m 14s (- 13m 17s) (435 43%) 0.0533\n","10m 21s (- 13m 10s) (440 44%) 0.0540\n","10m 28s (- 13m 3s) (445 44%) 0.0543\n","10m 35s (- 12m 56s) (450 45%) 0.0557\n","10m 42s (- 12m 49s) (455 45%) 0.0553\n","10m 49s (- 12m 42s) (460 46%) 0.0597\n","10m 56s (- 12m 35s) (465 46%) 0.0550\n","11m 3s (- 12m 27s) (470 47%) 0.0575\n","11m 10s (- 12m 20s) (475 47%) 0.0705\n","11m 17s (- 12m 13s) (480 48%) 0.0592\n","11m 24s (- 12m 6s) (485 48%) 0.0588\n","11m 31s (- 11m 59s) (490 49%) 0.0571\n","11m 38s (- 11m 52s) (495 49%) 0.0548\n","11m 45s (- 11m 45s) (500 50%) 0.0552\n","11m 52s (- 11m 38s) (505 50%) 0.0528\n","11m 59s (- 11m 31s) (510 51%) 0.0533\n","12m 6s (- 11m 24s) (515 51%) 0.0553\n","12m 13s (- 11m 17s) (520 52%) 0.0528\n","12m 20s (- 11m 10s) (525 52%) 0.0549\n","12m 27s (- 11m 3s) (530 53%) 0.0549\n","12m 34s (- 10m 56s) (535 53%) 0.0551\n","12m 41s (- 10m 48s) (540 54%) 0.0554\n","12m 48s (- 10m 41s) (545 54%) 0.0552\n","12m 55s (- 10m 34s) (550 55%) 0.0588\n","13m 2s (- 10m 27s) (555 55%) 0.0641\n","13m 9s (- 10m 20s) (560 56%) 0.0559\n","13m 17s (- 10m 13s) (565 56%) 0.0554\n","13m 24s (- 10m 6s) (570 56%) 0.0528\n","13m 31s (- 9m 59s) (575 57%) 0.0552\n","13m 38s (- 9m 52s) (580 57%) 0.0556\n","13m 45s (- 9m 45s) (585 58%) 0.0551\n","13m 52s (- 9m 38s) (590 59%) 0.0528\n","13m 59s (- 9m 31s) (595 59%) 0.0554\n","14m 6s (- 9m 24s) (600 60%) 0.0544\n","14m 12s (- 9m 16s) (605 60%) 0.0556\n","14m 20s (- 9m 9s) (610 61%) 0.0540\n","14m 26s (- 9m 2s) (615 61%) 0.0535\n","14m 33s (- 8m 55s) (620 62%) 0.0593\n","14m 41s (- 8m 48s) (625 62%) 0.0558\n","14m 48s (- 8m 41s) (630 63%) 0.0530\n","14m 55s (- 8m 34s) (635 63%) 0.0555\n","15m 2s (- 8m 27s) (640 64%) 0.0565\n","15m 9s (- 8m 20s) (645 64%) 0.0533\n","15m 16s (- 8m 13s) (650 65%) 0.0521\n","15m 23s (- 8m 6s) (655 65%) 0.0592\n","15m 30s (- 7m 59s) (660 66%) 0.0551\n","15m 37s (- 7m 52s) (665 66%) 0.0573\n","15m 44s (- 7m 45s) (670 67%) 0.0525\n","15m 51s (- 7m 38s) (675 67%) 0.0574\n","15m 58s (- 7m 31s) (680 68%) 0.0535\n","16m 6s (- 7m 24s) (685 68%) 0.0528\n","16m 13s (- 7m 17s) (690 69%) 0.0583\n","16m 20s (- 7m 10s) (695 69%) 0.0583\n","16m 27s (- 7m 3s) (700 70%) 0.0606\n","16m 34s (- 6m 56s) (705 70%) 0.0540\n","16m 42s (- 6m 49s) (710 71%) 0.0534\n","16m 49s (- 6m 42s) (715 71%) 0.0553\n","16m 56s (- 6m 35s) (720 72%) 0.0530\n","17m 3s (- 6m 28s) (725 72%) 0.0526\n","17m 10s (- 6m 21s) (730 73%) 0.0566\n","17m 17s (- 6m 14s) (735 73%) 0.0572\n","17m 24s (- 6m 7s) (740 74%) 0.0570\n","17m 31s (- 6m 0s) (745 74%) 0.0556\n","17m 38s (- 5m 52s) (750 75%) 0.0592\n","17m 46s (- 5m 45s) (755 75%) 0.0554\n","17m 53s (- 5m 38s) (760 76%) 0.0526\n","18m 0s (- 5m 31s) (765 76%) 0.0547\n","18m 6s (- 5m 24s) (770 77%) 0.0573\n","18m 13s (- 5m 17s) (775 77%) 0.0577\n","18m 20s (- 5m 10s) (780 78%) 0.0515\n","18m 27s (- 5m 3s) (785 78%) 0.0551\n","18m 34s (- 4m 56s) (790 79%) 0.0541\n","18m 41s (- 4m 49s) (795 79%) 0.0556\n","18m 48s (- 4m 42s) (800 80%) 0.0543\n","18m 55s (- 4m 35s) (805 80%) 0.0537\n","19m 2s (- 4m 28s) (810 81%) 0.0540\n","19m 9s (- 4m 20s) (815 81%) 0.0546\n","19m 16s (- 4m 13s) (820 82%) 0.0593\n","19m 23s (- 4m 6s) (825 82%) 0.0570\n","19m 30s (- 3m 59s) (830 83%) 0.0531\n","19m 37s (- 3m 52s) (835 83%) 0.0560\n","19m 44s (- 3m 45s) (840 84%) 0.0554\n","19m 51s (- 3m 38s) (845 84%) 0.0574\n","19m 58s (- 3m 31s) (850 85%) 0.0571\n","20m 5s (- 3m 24s) (855 85%) 0.0548\n","20m 12s (- 3m 17s) (860 86%) 0.0549\n","20m 19s (- 3m 10s) (865 86%) 0.0608\n","20m 27s (- 3m 3s) (870 87%) 0.0541\n","20m 34s (- 2m 56s) (875 87%) 0.0541\n","20m 41s (- 2m 49s) (880 88%) 0.0551\n","20m 48s (- 2m 42s) (885 88%) 0.0577\n","20m 55s (- 2m 35s) (890 89%) 0.0562\n","21m 2s (- 2m 28s) (895 89%) 0.0599\n","21m 9s (- 2m 21s) (900 90%) 0.0559\n","21m 16s (- 2m 14s) (905 90%) 0.0561\n","21m 23s (- 2m 6s) (910 91%) 0.0516\n","21m 30s (- 1m 59s) (915 91%) 0.0536\n","21m 37s (- 1m 52s) (920 92%) 0.0551\n","21m 45s (- 1m 45s) (925 92%) 0.0519\n","21m 52s (- 1m 38s) (930 93%) 0.0567\n","21m 59s (- 1m 31s) (935 93%) 0.0578\n","22m 6s (- 1m 24s) (940 94%) 0.0581\n","22m 13s (- 1m 17s) (945 94%) 0.0536\n","22m 20s (- 1m 10s) (950 95%) 0.0572\n","22m 27s (- 1m 3s) (955 95%) 0.0565\n","22m 34s (- 0m 56s) (960 96%) 0.0528\n","22m 41s (- 0m 49s) (965 96%) 0.0543\n","22m 48s (- 0m 42s) (970 97%) 0.0544\n","22m 55s (- 0m 35s) (975 97%) 0.0560\n","23m 2s (- 0m 28s) (980 98%) 0.0548\n","23m 9s (- 0m 21s) (985 98%) 0.0578\n","23m 16s (- 0m 14s) (990 99%) 0.0587\n","23m 23s (- 0m 7s) (995 99%) 0.0534\n","23m 30s (- 0m 0s) (1000 100%) 0.0559\n"]}],"source":["hidden_size = 256\n","batch_size = 16\n","\n","input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n","\n","encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n","decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n","\n","train(train_dataloader, encoder,decoder, n_epochs = 1000, print_every=5, plot_every=5)"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T20:27:04.461113Z","iopub.status.busy":"2024-04-20T20:27:04.460731Z","iopub.status.idle":"2024-04-20T20:27:04.534958Z","shell.execute_reply":"2024-04-20T20:27:04.533974Z","shell.execute_reply.started":"2024-04-20T20:27:04.461083Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["> thats not my wife\n","= هذه ليست زوجتي\n","< هذه ليست زوجتي <EOS>\n","\n","> do not disturb her\n","= لا تزعجها\n","< لا تزعجها <EOS>\n","\n","> this flower is the most beautiful of all flowers\n","= هذه الوردة هي اجمل الورود على الاطلاق\n","< هذه الوردة هي اجمل الورود على الاطلاق <EOS>\n","\n","> this car is his\n","= هذه السيارة له\n","< هذه السيارة له <EOS>\n","\n","> im really hungry\n","= انا في غاية الجوع\n","< انا جايع جدا <EOS>\n","\n","> i dont mind either way\n","= لا امانع في كلتا الحالتين\n","< لا امانع في كلتا الحالتين <EOS>\n","\n","> i also found this\n","= انا وجدت ذلك ايضا\n","< انا وجدت ذلك ايضا <EOS>\n","\n","> i know your father\n","= انا اعرف اباك\n","< انا اعرف اباك <EOS>\n","\n","> you cant run my life\n","= لا يمكنك التحكم في حياتي\n","< لا يمكنك التحكم في حياتي <EOS>\n","\n","> im sorry\n","= انا اسف\n","< انا اسف <EOS>\n","\n"]}],"source":["encoder.eval()\n","decoder.eval()\n","evaluateRandomly(encoder, decoder)"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T17:34:45.895333Z","iopub.status.busy":"2024-04-20T17:34:45.894739Z","iopub.status.idle":"2024-04-20T17:34:45.964908Z","shell.execute_reply":"2024-04-20T17:34:45.964016Z","shell.execute_reply.started":"2024-04-20T17:34:45.895303Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["> i dont think tom would want to do that\n","= لا اظنن توم يريد فعل ذلك\n","< لا اظنن توم يريد فعل ذلك <EOS>\n","\n","> i read his book\n","= انا اقرا كتابه\n","< انا اقرا كتابه على الاطلاق <EOS>\n","\n","> im sure that she will come back soon\n","= انا متاكد من انها ستعود قريبا\n","< انا متاكد من انها ستعود قريبا <EOS>\n","\n","> im serious\n","= انا لا امزح\n","< انا لا احاول ان <EOS>\n","\n","> im really hungry\n","= انا جايع جدا\n","< انا جايع جدا في الصباح <EOS>\n","\n","> i cant see anything\n","= لا ارى شييا\n","< لا استطيع ابتكار ارى <EOS>\n","\n","> im trying to sleep\n","= انا احاول ان انام\n","< انا احاول ان انام احاول <EOS>\n","\n","> no one shall be arbitrarily deprived of his property\n","= لا يجوز تجريد احد من ملكه تعسفا\n","< لا يجوز تجريد احد من ملكه تعسفا <EOS>\n","\n","> i like animals for example cats and dogs\n","= انا احب الحيوانات على سبيل المثال القطط والكلاب\n","< انا احب الحيوانات على سبيل المثال القطط والكلاب <EOS>\n","\n","> i dont know when hell be here\n","= لا اعرف متى سيكون هنا\n","< لا اعرف متى سيكون هنا <EOS>\n","\n"]}],"source":["encoder.eval()\n","decoder.eval()\n","evaluateRandomly(encoder, decoder)"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T20:28:01.540226Z","iopub.status.busy":"2024-04-20T20:28:01.539513Z","iopub.status.idle":"2024-04-20T20:28:01.567297Z","shell.execute_reply":"2024-04-20T20:28:01.566292Z","shell.execute_reply.started":"2024-04-20T20:28:01.540191Z"},"trusted":true},"outputs":[],"source":["# Save encoder\n","torch.save(encoder.state_dict(), './english_ara_encoder.pth')\n","\n","# Save decoder\n","torch.save(decoder.state_dict(), './english_ara_decoder.pth')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-20T17:34:40.863257Z","iopub.status.idle":"2024-04-20T17:34:40.863601Z","shell.execute_reply":"2024-04-20T17:34:40.863454Z","shell.execute_reply.started":"2024-04-20T17:34:40.863440Z"},"trusted":true},"outputs":[],"source":["encoder.load_state_dict(torch.load('./eng_to_ara_encoder.pth'))\n","decoder.load_state_dict(torch.load('./eng_to_ara_decoder.pth'))\n","\n","def evaluateOneSentence(encoder, decoder, sentence, input_lang, output_lang):\n","    print('>', sentence)\n","    output_words, _ = evaluate(encoder, decoder, sentence, input_lang, output_lang)\n","    output_sentence = ' '.join(output_words)\n","    print('<', output_sentence)\n","\n","# Example usage:\n","input_lang, output_lang, pairs = prepareData('english', 'arabic', False)\n","encoder = encoder # initialize your encoder\n","decoder = decoder # initialize your decoder\n","# input_sentence = \"انا لا اشعر بالعطش\"\n","input_sentence = \"know what\"\n","evaluateOneSentence(encoder, decoder, input_sentence, input_lang, output_lang)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-20T17:34:40.865293Z","iopub.status.idle":"2024-04-20T17:34:40.865631Z","shell.execute_reply":"2024-04-20T17:34:40.865484Z","shell.execute_reply.started":"2024-04-20T17:34:40.865470Z"},"trusted":true},"outputs":[],"source":["import random\n","import nltk\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","\n","def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n","    with torch.no_grad():\n","        input_tensor = tensorFromSentence(input_lang, sentence)\n","\n","        encoder_outputs, encoder_hidden = encoder(input_tensor)\n","        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n","\n","        _, topi = decoder_outputs.topk(1)\n","        decoded_ids = topi.squeeze()\n","\n","        decoded_words = []\n","        for idx in decoded_ids:\n","            if idx.item() == EOS_token:\n","                decoded_words.append('<EOS>')\n","                break\n","            decoded_words.append(output_lang.index2word[idx.item()])\n","    return decoded_words, decoder_attn\n","\n","def evaluateRandomly(encoder, decoder, n=10):\n","    references = []  # List to store reference translations\n","    candidates = []  # List to store candidate translations\n","    \n","    for i in range(n):\n","        pair = random.choice(pairs)\n","        print('>', pair[0])\n","        print('=', pair[1])\n","        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n","        output_sentence = ' '.join(output_words)\n","        print('<', output_sentence)\n","        print('')\n","        \n","        references.append(pair[1].split())  # Add reference translation to references list\n","        candidates.append(output_words[:-1])  # Remove <EOS> token from candidate translation and add to candidates list\n","    \n","    # Calculate BLEU score with smoothing function\n","    smoothing_function = SmoothingFunction().method4\n","    bleu_score = nltk.translate.bleu_score.corpus_bleu(references, candidates)\n","    print(\"BLEU Score:\", bleu_score)\n","\n","encoder.eval()\n","decoder.eval()\n","evaluateRandomly(encoder, decoder)\n"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T17:45:56.017908Z","iopub.status.busy":"2024-04-20T17:45:56.017254Z","iopub.status.idle":"2024-04-20T17:45:56.022905Z","shell.execute_reply":"2024-04-20T17:45:56.021985Z","shell.execute_reply.started":"2024-04-20T17:45:56.017880Z"},"trusted":true},"outputs":[],"source":["import random\n","\n","def evaluateSpecificSentence(encoder, decoder, sentence, input_lang, output_lang):\n","    print('>', sentence)\n","    output_words, _ = evaluate(encoder, decoder, sentence, input_lang, output_lang)\n","    output_sentence = ' '.join(output_words)\n","    print('<', output_sentence)\n","    print('')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T17:46:06.214150Z","iopub.status.busy":"2024-04-20T17:46:06.213324Z","iopub.status.idle":"2024-04-20T17:46:06.227908Z","shell.execute_reply":"2024-04-20T17:46:06.226928Z","shell.execute_reply.started":"2024-04-20T17:46:06.214117Z"},"trusted":true},"outputs":[],"source":["encoder.eval()\n","decoder.eval()\n","evaluateSpecificSentence(encoder, decoder,\"no please\",input_lang,output_lang)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4833694,"sourceId":8168185,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
